version: "3.8"

services:
  voice-llm-stack:
    build:
      context: .
      dockerfile: base/Dockerfile
    image: ${DOCKERHUB_USERNAME:-schnicklbob}/quickpod-voice-llm:latest
    container_name: voice-llm-stack
    gpus: all
    ports:
      - "22:22"
      - "8000:8000"
      - "11434:11434"
      - "7851:7851"
      - "8686:8686"
    environment:
      - ST_PORT=8000
      - OLLAMA_PORT=11434
      - ALLTALK_PORT=7851
      - HEALTH_PORT=8686
    volumes:
      - ./sillytavern:/workspace/sillytavern:rw
      - ./ollama:/root/.ollama:rw
      - ./alltalk:/workspace/alltalk_tts:rw
      - ./venv:/opt/venv:rw
      - ./logs:/var/log/quickpod:rw
    shm_size: "8gb"
    restart: unless-stopped

  image-api:
    build:
      context: .
      dockerfile: docker/image_api.Dockerfile
    container_name: image-api
    gpus: all
    environment:
      - HF_TOKEN=${HF_TOKEN:-}
      - MODEL_ID=lodestones/Chroma
      - DEVICE=cuda
      - IMAGE_DIR=/app/generated_images
      - MAX_BATCH=1
    ports:
      - "9000:9000"
    volumes:
      - ./image_store:/app/generated_images:rw
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:9000/health || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 6
      start_period: 20s
